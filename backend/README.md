# ðŸ¤– Backend IA - Volquetes RoldÃ¡n

Sistema de IA conversacional con **Ollama + Llama 3** para asistente virtual empresarial.

## ðŸ“‹ CaracterÃ­sticas

- âœ… **100% Gratuito**: Sin costos de APIs externas
- âœ… **IA Local**: Ollama + Llama 3 8B
- âœ… **Base de Datos Real**: Consultas a MySQL
- âœ… **Sin Alucinaciones**: Solo responde con datos verificados
- âœ… **Optimizado**: Para VPS con 8GB RAM
- âœ… **Memoria Conversacional**: Ãšltimos 3 mensajes

## ðŸš€ Inicio RÃ¡pido

### 1. Instalar Dependencias

```bash
cd backend
npm install
```

### 2. Configurar Variables de Entorno

```bash
cp .env.example .env
nano .env
```

Configurar:
- `DB_USER` y `DB_PASSWORD`: Credenciales MySQL
- `DB_NAME`: `volquetes_roldan`
- `OLLAMA_URL`: `http://localhost:11434`

#### OpciÃ³n Grok (xAI)

Si querÃ©s usar **Grok** en lugar de Ollama:

```env
AI_PROVIDER=grok
OPENAI_BASE_URL=https://api.x.ai/v1
OPENAI_API_KEY=tu_api_key
OPENAI_MODEL=grok-2-latest
```

Este backend debe correr en un servidor con Node (VPS, Render, Railway, etc.). No funciona en hosting compartido sin Node.

#### OpciÃ³n Groq

Si querÃ©s usar **Groq**:

```env
AI_PROVIDER=groq
GROQ_BASE_URL=https://api.groq.com/openai/v1
GROQ_API_KEY=tu_api_key
GROQ_MODEL=llama3-8b-8192
```

PodÃ©s cambiar `GROQ_MODEL` por cualquier modelo disponible en tu cuenta.

### 3. Instalar y Configurar MySQL

```bash
# Instalar MySQL (si no lo tienes)
sudo apt install mysql-server

# Crear base de datos e importar esquema
mysql -u root -p < ../database/schema.sql

# Importar datos de ejemplo
mysql -u root -p < ../database/seed.sql
```

### 4. Instalar Ollama

```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelo Llama 3 8B (4.7GB)
ollama pull llama3:8b

# Iniciar servicio
ollama serve
```

**Dejar Ollama corriendo en una terminal separada**

### 5. Iniciar Backend

```bash
# En otra terminal
cd backend
npm start
```

El servidor estarÃ¡ en: `http://localhost:3001`

## ðŸ§ª Probar el Sistema

### Test de Health Check

```bash
curl http://localhost:3001/health
```

### Test de Ollama

```bash
curl http://localhost:3001/api/chat/health
```

### Test de Chat

```bash
curl -X POST http://localhost:3001/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Â¿CuÃ¡nto cuesta alquilar un volquete?"
  }'
```

## ðŸ“š API Endpoints

### POST /api/chat

Procesa mensaje del usuario y devuelve respuesta de IA.

**Request:**
```json
{
  "message": "Â¿QuÃ© tamaÃ±os de volquetes tienen?",
  "conversationHistory": [
    { "role": "user", "content": "Hola" },
    { "role": "assistant", "content": "Â¡Hola! Â¿En quÃ© puedo ayudarte?" }
  ]
}
```

**Response:**
```json
{
  "response": "Tenemos 4 tamaÃ±os: Chico (1.5mÂ³), Mediano (3mÂ³), Grande (6mÂ³) y con Barandas (7mÂ³)...",
  "timestamp": "2026-02-11T05:00:00.000Z",
  "model": "llama3:8b",
  "intent": "tamanos",
  "sources": "database + IA"
}
```

### GET /api/chat/health

Verifica estado del backend y Ollama.

**Response:**
```json
{
  "backend": "OK",
  "ollama": {
    "status": "OK",
    "models": ["llama3:8b"],
    "url": "http://localhost:11434"
  },
  "timestamp": "2026-02-11T05:00:00.000Z"
}
```

## ðŸ—‚ï¸ Estructura del Proyecto

```
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.js              # Servidor Express principal
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ database.js        # ConfiguraciÃ³n MySQL
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ chat.js            # Endpoint /api/chat
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ chat.service.js    # LÃ³gica principal del chat
â”‚   â”‚   â”œâ”€â”€ ollama.service.js  # ComunicaciÃ³n con Ollama
â”‚   â”‚   â”œâ”€â”€ database.service.js # Consultas SQL
â”‚   â”‚   â””â”€â”€ prompt.service.js  # ConstrucciÃ³n de prompts
â”‚   â””â”€â”€ middleware/
â”‚       â””â”€â”€ errorHandler.js    # Manejo de errores
â”œâ”€â”€ .env                       # Variables de entorno
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Optimizar Ollama para VPS

Editar `/etc/systemd/system/ollama.service`:

```ini
[Service]
Environment="OLLAMA_NUM_PARALLEL=1"
Environment="OLLAMA_MAX_LOADED_MODELS=1"
Environment="OLLAMA_FLASH_ATTENTION=1"
```

```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### Ajustar ParÃ¡metros de IA

En `.env`:

```env
# Respuestas mÃ¡s creativas (0.1 - 1.0)
OLLAMA_TEMPERATURE=0.5

# Respuestas mÃ¡s largas (100 - 2000)
OLLAMA_MAX_TOKENS=800
```

## ðŸ› Troubleshooting

### Error: "ECONNREFUSED localhost:11434"

**Problema**: Ollama no estÃ¡ corriendo.

**SoluciÃ³n**:
```bash
# Iniciar Ollama
ollama serve

# O como servicio
sudo systemctl start ollama
```

### Error: "Modelo no encontrado"

**Problema**: Modelo Llama 3 no descargado.

**SoluciÃ³n**:
```bash
ollama pull llama3:8b
ollama list  # Verificar
```

### Error: "ER_ACCESS_DENIED_ERROR"

**Problema**: Credenciales MySQL incorrectas.

**SoluciÃ³n**:
```bash
# Verificar .env
cat .env | grep DB_

# Crear usuario MySQL
mysql -u root -p
CREATE USER 'volquetes_user'@'localhost' IDENTIFIED BY 'tu_password';
GRANT ALL PRIVILEGES ON volquetes_roldan.* TO 'volquetes_user'@'localhost';
FLUSH PRIVILEGES;
```

### Respuestas muy lentas

**SoluciÃ³n 1**: Reducir tokens mÃ¡ximos
```env
OLLAMA_MAX_TOKENS=300
```

**SoluciÃ³n 2**: Usar Mistral (mÃ¡s rÃ¡pido)
```bash
ollama pull mistral:7b
```

Cambiar en `.env`:
```env
OLLAMA_MODEL=mistral:7b
```

## ðŸ“Š Monitoreo

```bash
# Ver logs de Ollama
sudo journalctl -u ollama -f

# Ver logs del backend
npm start | tee backend.log

# Monitorear recursos
htop
```

## ðŸš€ Deployment en ProducciÃ³n

Ver documentaciÃ³n completa en:
- `docs/AI_ARCHITECTURE.md`
- `docs/AI_ARCHITECTURE_PART2.md`

## ðŸ“ž Soporte

Si encuentras problemas:

1. Verifica que todos los servicios estÃ©n corriendo
2. Revisa los logs
3. Consulta la documentaciÃ³n completa en `/docs`

---

**Â¡Sistema listo! ðŸŽ‰**
